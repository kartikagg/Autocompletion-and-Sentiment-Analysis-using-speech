{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Auto-Complete.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vtAeyozzkzRL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "import pickle\n",
        "import numpy as np\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "EHWkhkhMlESX",
        "outputId": "f9db116b-dcd2-4058-f1cf-37bd10553b4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-1ae8bbfa-b673-4ec2-8e25-046b2e047947\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-1ae8bbfa-b673-4ec2-8e25-046b2e047947\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving 1342-0.txt to 1342-0.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file = open(\"/content/1342-0.txt\", \"r\", encoding = \"utf8\")\n",
        "\n",
        "# store file in list\n",
        "lines = []\n",
        "for i in file:\n",
        "    lines.append(i)\n",
        "\n",
        "# Convert list to string\n",
        "data = \"\"\n",
        "for i in lines:\n",
        "  data = ' '. join(lines) \n",
        "\n",
        "#replace unnecessary stuff with space\n",
        "data = data.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('“','').replace('”','')  #new line, carriage return, unicode character --> replace by space\n",
        "\n",
        "#remove unnecessary spaces \n",
        "data = data.split()\n",
        "data = ' '.join(data)\n",
        "data[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 160
        },
        "id": "IktM5P0plPF0",
        "outputId": "5d5a95fa-a242-43c2-b311-20f7979630b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'The Project Gutenberg eBook of Pride and Prejudice, by Jane Austen This eBook is for the use of anyone anywhere in the United States and most other parts of the world at no cost and with almost no restrictions whatsoever. You may copy it, give it away or re-use it under the terms of the Project Gutenberg License included with this eBook or online at www.gutenberg.org. If you are not located in the United States, you will have to check the laws of the country where you are located before using this eBook. Title: Pride and Prejudice Author: Jane Austen Release Date: June, 1998 [eBook #1342] [Most recently updated: August 23, 2021] Language: English Character set encoding: UTF-8 Produced by: Anonymous Volunteers and David Widger *** START OF THE PROJECT GUTENBERG EBOOK PRIDE AND PREJUDICE *** THERE IS AN ILLUSTRATED EDITION OF THIS TITLE WHICH MAY VIEWED AT EBOOK [# 42671 ] cover Pride and Prejudice By Jane Austen CONTENTS Chapter 1 Chapter 2 Chapter 3 Chapter 4 Chapter 5 Chapter 6 Chapte'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts([data])\n",
        "\n",
        "# saving the tokenizer for predict function\n",
        "pickle.dump(tokenizer, open('token.pkl', 'wb'))\n",
        "\n",
        "sequence_data = tokenizer.texts_to_sequences([data])[0]\n",
        "sequence_data[:15]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SHf4FienlcpS",
        "outputId": "6f73d66f-95bc-4f9f-e0ca-bf4fbe23fda3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1, 176, 158, 916, 3, 321, 4, 1171, 30, 72, 2534, 41, 916, 23, 21]"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "print(vocab_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kG5WJfGqlsHy",
        "outputId": "279ae218-6f31-4723-c419-f974ca74481f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "7030\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = []\n",
        "\n",
        "for i in range(3, len(sequence_data)):\n",
        "    words = sequence_data[i-3:i+1]\n",
        "    sequences.append(words)\n",
        "    \n",
        "print(\"The Length of sequences are: \", len(sequences))\n",
        "sequences = np.array(sequences)\n",
        "sequences[:10]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n5CD4GhVlup8",
        "outputId": "8880cbdb-bee0-4a57-f82d-914f42091a7d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The Length of sequences are:  125306\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[   1,  176,  158,  916],\n",
              "       [ 176,  158,  916,    3],\n",
              "       [ 158,  916,    3,  321],\n",
              "       [ 916,    3,  321,    4],\n",
              "       [   3,  321,    4, 1171],\n",
              "       [ 321,    4, 1171,   30],\n",
              "       [   4, 1171,   30,   72],\n",
              "       [1171,   30,   72, 2534],\n",
              "       [  30,   72, 2534,   41],\n",
              "       [  72, 2534,   41,  916]])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X = []\n",
        "y = []\n",
        "\n",
        "for i in sequences:\n",
        "    X.append(i[0:3])\n",
        "    y.append(i[3])\n",
        "    \n",
        "X = np.array(X)\n",
        "y = np.array(y)"
      ],
      "metadata": {
        "id": "fgIu_U9OlyXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Data: \", X[:10])\n",
        "print(\"Response: \", y[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JRrF6u3zl6vz",
        "outputId": "b15b20a5-f114-433c-c9f5-e0c53ed88dd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data:  [[   1  176  158]\n",
            " [ 176  158  916]\n",
            " [ 158  916    3]\n",
            " [ 916    3  321]\n",
            " [   3  321    4]\n",
            " [ 321    4 1171]\n",
            " [   4 1171   30]\n",
            " [1171   30   72]\n",
            " [  30   72 2534]\n",
            " [  72 2534   41]]\n",
            "Response:  [ 916    3  321    4 1171   30   72 2534   41  916]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = to_categorical(y, num_classes=vocab_size)\n",
        "y[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e8_4XGpTl_MF",
        "outputId": "84408834-cf08-45ef-c085-d308d69a3570"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, 10, input_length=3))\n",
        "model.add(LSTM(1000, return_sequences=True))\n",
        "model.add(LSTM(1000))\n",
        "model.add(Dense(1000, activation=\"relu\"))\n",
        "model.add(Dense(vocab_size, activation=\"softmax\"))"
      ],
      "metadata": {
        "id": "ovCeE43MmIQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "checkpoint = ModelCheckpoint(\"next_words.h5\", monitor='loss', verbose=1, save_best_only=True)\n",
        "model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(learning_rate=0.001))\n",
        "model.fit(X, y, epochs=70, batch_size=64, callbacks=[checkpoint])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MggOXOl1mMNA",
        "outputId": "3c3389d9-e370-47f4-ee47-d79acc972fc8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 6.2258\n",
            "Epoch 1: loss improved from inf to 6.22538, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 37s 15ms/step - loss: 6.2254\n",
            "Epoch 2/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 5.6264\n",
            "Epoch 2: loss improved from 6.22538 to 5.62618, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 5.6262\n",
            "Epoch 3/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 5.3022\n",
            "Epoch 3: loss improved from 5.62618 to 5.30237, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 5.3024\n",
            "Epoch 4/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 5.0712\n",
            "Epoch 4: loss improved from 5.30237 to 5.07117, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 5.0712\n",
            "Epoch 5/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 4.8657\n",
            "Epoch 5: loss improved from 5.07117 to 4.86599, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 4.8660\n",
            "Epoch 6/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 4.6675\n",
            "Epoch 6: loss improved from 4.86599 to 4.66760, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 4.6676\n",
            "Epoch 7/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 4.4721\n",
            "Epoch 7: loss improved from 4.66760 to 4.47222, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 4.4722\n",
            "Epoch 8/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 4.2752\n",
            "Epoch 8: loss improved from 4.47222 to 4.27545, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 4.2755\n",
            "Epoch 9/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 4.0721\n",
            "Epoch 9: loss improved from 4.27545 to 4.07213, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 4.0721\n",
            "Epoch 10/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 3.8583\n",
            "Epoch 10: loss improved from 4.07213 to 3.85846, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 3.8585\n",
            "Epoch 11/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 3.6378\n",
            "Epoch 11: loss improved from 3.85846 to 3.63819, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 3.6382\n",
            "Epoch 12/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 3.4100\n",
            "Epoch 12: loss improved from 3.63819 to 3.40999, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 3.4100\n",
            "Epoch 13/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 3.1749\n",
            "Epoch 13: loss improved from 3.40999 to 3.17489, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 3.1749\n",
            "Epoch 14/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 2.9306\n",
            "Epoch 14: loss improved from 3.17489 to 2.93079, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 2.9308\n",
            "Epoch 15/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 2.6844\n",
            "Epoch 15: loss improved from 2.93079 to 2.68443, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 2.6844\n",
            "Epoch 16/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 2.4340\n",
            "Epoch 16: loss improved from 2.68443 to 2.43398, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 2.4340\n",
            "Epoch 17/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 2.1868\n",
            "Epoch 17: loss improved from 2.43398 to 2.18726, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 2.1873\n",
            "Epoch 18/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 1.9482\n",
            "Epoch 18: loss improved from 2.18726 to 1.94855, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 1.9486\n",
            "Epoch 19/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 1.7295\n",
            "Epoch 19: loss improved from 1.94855 to 1.72969, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 1.7297\n",
            "Epoch 20/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 1.5333\n",
            "Epoch 20: loss improved from 1.72969 to 1.53341, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 1.5334\n",
            "Epoch 21/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 1.3638\n",
            "Epoch 21: loss improved from 1.53341 to 1.36391, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 1.3639\n",
            "Epoch 22/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 1.2229\n",
            "Epoch 22: loss improved from 1.36391 to 1.22304, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 1.2230\n",
            "Epoch 23/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 1.1055\n",
            "Epoch 23: loss improved from 1.22304 to 1.10563, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 1.1056\n",
            "Epoch 24/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 1.0111\n",
            "Epoch 24: loss improved from 1.10563 to 1.01110, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 1.0111\n",
            "Epoch 25/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.9385\n",
            "Epoch 25: loss improved from 1.01110 to 0.93871, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.9387\n",
            "Epoch 26/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.8759\n",
            "Epoch 26: loss improved from 0.93871 to 0.87600, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 0.8760\n",
            "Epoch 27/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.8313\n",
            "Epoch 27: loss improved from 0.87600 to 0.83147, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 0.8315\n",
            "Epoch 28/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.7917\n",
            "Epoch 28: loss improved from 0.83147 to 0.79186, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 0.7919\n",
            "Epoch 29/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.7566\n",
            "Epoch 29: loss improved from 0.79186 to 0.75671, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.7567\n",
            "Epoch 30/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.7230\n",
            "Epoch 30: loss improved from 0.75671 to 0.72301, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.7230\n",
            "Epoch 31/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.7030\n",
            "Epoch 31: loss improved from 0.72301 to 0.70308, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 0.7031\n",
            "Epoch 32/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.6841\n",
            "Epoch 32: loss improved from 0.70308 to 0.68406, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.6841\n",
            "Epoch 33/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.6632\n",
            "Epoch 33: loss improved from 0.68406 to 0.66324, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.6632\n",
            "Epoch 34/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.6482\n",
            "Epoch 34: loss improved from 0.66324 to 0.64843, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.6484\n",
            "Epoch 35/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.6332\n",
            "Epoch 35: loss improved from 0.64843 to 0.63313, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.6331\n",
            "Epoch 36/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.6163\n",
            "Epoch 36: loss improved from 0.63313 to 0.61639, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.6164\n",
            "Epoch 37/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.6062\n",
            "Epoch 37: loss improved from 0.61639 to 0.60637, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.6064\n",
            "Epoch 38/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.5944\n",
            "Epoch 38: loss improved from 0.60637 to 0.59441, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5944\n",
            "Epoch 39/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.5821\n",
            "Epoch 39: loss improved from 0.59441 to 0.58213, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5821\n",
            "Epoch 40/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.5760\n",
            "Epoch 40: loss improved from 0.58213 to 0.57599, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5760\n",
            "Epoch 41/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.5646\n",
            "Epoch 41: loss improved from 0.57599 to 0.56470, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5647\n",
            "Epoch 42/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.5560\n",
            "Epoch 42: loss improved from 0.56470 to 0.55612, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5561\n",
            "Epoch 43/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.5473\n",
            "Epoch 43: loss improved from 0.55612 to 0.54731, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5473\n",
            "Epoch 44/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.5451\n",
            "Epoch 44: loss improved from 0.54731 to 0.54518, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5452\n",
            "Epoch 45/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.5352\n",
            "Epoch 45: loss improved from 0.54518 to 0.53523, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5352\n",
            "Epoch 46/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.5259\n",
            "Epoch 46: loss improved from 0.53523 to 0.52599, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5260\n",
            "Epoch 47/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.5202\n",
            "Epoch 47: loss improved from 0.52599 to 0.52047, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5205\n",
            "Epoch 48/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.5160\n",
            "Epoch 48: loss improved from 0.52047 to 0.51615, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5161\n",
            "Epoch 49/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.5114\n",
            "Epoch 49: loss improved from 0.51615 to 0.51145, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5114\n",
            "Epoch 50/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.5032\n",
            "Epoch 50: loss improved from 0.51145 to 0.50321, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5032\n",
            "Epoch 51/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.5020\n",
            "Epoch 51: loss improved from 0.50321 to 0.50191, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.5019\n",
            "Epoch 52/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4923\n",
            "Epoch 52: loss improved from 0.50191 to 0.49226, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4923\n",
            "Epoch 53/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.4900\n",
            "Epoch 53: loss improved from 0.49226 to 0.48996, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4900\n",
            "Epoch 54/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.4889\n",
            "Epoch 54: loss improved from 0.48996 to 0.48898, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4890\n",
            "Epoch 55/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4796\n",
            "Epoch 55: loss improved from 0.48898 to 0.47956, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4796\n",
            "Epoch 56/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4760\n",
            "Epoch 56: loss improved from 0.47956 to 0.47604, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4760\n",
            "Epoch 57/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.4752\n",
            "Epoch 57: loss improved from 0.47604 to 0.47542, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4754\n",
            "Epoch 58/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.4701\n",
            "Epoch 58: loss improved from 0.47542 to 0.47015, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4701\n",
            "Epoch 59/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.4676\n",
            "Epoch 59: loss improved from 0.47015 to 0.46772, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4677\n",
            "Epoch 60/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4588\n",
            "Epoch 60: loss improved from 0.46772 to 0.45878, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4588\n",
            "Epoch 61/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.4616\n",
            "Epoch 61: loss did not improve from 0.45878\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 0.4617\n",
            "Epoch 62/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.4561\n",
            "Epoch 62: loss improved from 0.45878 to 0.45611, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4561\n",
            "Epoch 63/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4544\n",
            "Epoch 63: loss improved from 0.45611 to 0.45444, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 16ms/step - loss: 0.4544\n",
            "Epoch 64/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4519\n",
            "Epoch 64: loss improved from 0.45444 to 0.45193, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4519\n",
            "Epoch 65/70\n",
            "1957/1958 [============================>.] - ETA: 0s - loss: 0.4493\n",
            "Epoch 65: loss improved from 0.45193 to 0.44925, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4492\n",
            "Epoch 66/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.4454\n",
            "Epoch 66: loss improved from 0.44925 to 0.44545, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4454\n",
            "Epoch 67/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.4430\n",
            "Epoch 67: loss improved from 0.44545 to 0.44318, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4432\n",
            "Epoch 68/70\n",
            "1955/1958 [============================>.] - ETA: 0s - loss: 0.4399\n",
            "Epoch 68: loss improved from 0.44318 to 0.43995, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4400\n",
            "Epoch 69/70\n",
            "1956/1958 [============================>.] - ETA: 0s - loss: 0.4399\n",
            "Epoch 69: loss did not improve from 0.43995\n",
            "1958/1958 [==============================] - 29s 15ms/step - loss: 0.4401\n",
            "Epoch 70/70\n",
            "1958/1958 [==============================] - ETA: 0s - loss: 0.4362\n",
            "Epoch 70: loss improved from 0.43995 to 0.43624, saving model to next_words.h5\n",
            "1958/1958 [==============================] - 30s 15ms/step - loss: 0.4362\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f52ea4086d0>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "import numpy as np\n",
        "import pickle\n",
        "\n",
        "# Load the model and tokenizer\n",
        "model = load_model('next_words.h5')\n",
        "tokenizer = pickle.load(open('token.pkl', 'rb'))\n",
        "\n",
        "def Predict_Next_Words(model, tokenizer, text):\n",
        "\n",
        "  sequence = tokenizer.texts_to_sequences([text])\n",
        "  sequence = np.array(sequence)\n",
        "  preds = np.argmax(model.predict(sequence))\n",
        "  predicted_word = \"\"\n",
        "  \n",
        "  for key, value in tokenizer.word_index.items():\n",
        "      if value == preds:\n",
        "          predicted_word = key\n",
        "          break\n",
        "  \n",
        "  print(predicted_word)\n",
        "  return predicted_word"
      ],
      "metadata": {
        "id": "Z3QxuuhpmT4p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while(True):\n",
        "  text = input(\"Enter your line: \")\n",
        "  \n",
        "  if text == \"0\":\n",
        "      print(\"Execution completed.....\")\n",
        "      break\n",
        "  \n",
        "  else:\n",
        "      try:\n",
        "          text = text.split(\" \")\n",
        "          text = text[-3:]\n",
        "          print(text)\n",
        "        \n",
        "          Predict_Next_Words(model, tokenizer, text)\n",
        "          \n",
        "      except Exception as e:\n",
        "        print(\"Error occurred: \",e)\n",
        "        continue"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k1TFqZPJmaZ4",
        "outputId": "cdf816d8-8943-4689-c1cc-0dacdc2f4170"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your line: a truth universally\n",
            "['a', 'truth', 'universally']\n",
            "acknowledged\n",
            "Enter your line: Bennet made no\n",
            "['Bennet', 'made', 'no']\n",
            "answer\n",
            "Enter your line: I see no\n",
            "['I', 'see', 'no']\n",
            "occasion\n",
            "Enter your line: i love\n",
            "['i', 'love']\n",
            "it\n",
            "Enter your line: the movie\n",
            "['the', 'movie']\n",
            "party\n",
            "Enter your line: comes into the\n",
            "['comes', 'into', 'the']\n",
            "neighbourhood\n",
            "Enter your line: when he comes into the\n",
            "['comes', 'into', 'the']\n",
            "neighbourhood\n",
            "Enter your line: 0\n",
            "Execution completed.....\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "c4RbRVTsmfRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}