# -*- coding: utf-8 -*-
"""Untitled8.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1OEb8sAMZ1C3RDk7wgd62vCAtnUgVW_UT
"""


import re
import nltk
import emoji

import pandas as pd
from autocorrect import Speller
from string import punctuation
from nltk.corpus import stopwords 
from nltk.stem import WordNetLemmatizer

from sklearn.svm import LinearSVC
from sklearn.neighbors import NearestCentroid
from sklearn.naive_bayes import BernoulliNB, ComplementNB, MultinomialNB, GaussianNB
from sklearn.linear_model import LogisticRegression, PassiveAggressiveClassifier, SGDClassifier, Perceptron

from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from sklearn.feature_extraction.text import CountVectorizer 
from sklearn.feature_extraction.text import TfidfTransformer

data0 = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding = "ISO-8859-1",
                    names=["sentiment", "ids", "date", "flag", "user", "text"])

data0

data1 = data0.copy()
data1 = data1[['sentiment', 'text']]

emojis = {':)': 'smile', ':-)': 'smile', ';d': 'wink', ':-E': 'vampire', ':(': 'sad', 
          ':-(': 'sad', ':-<': 'sad', ':P': 'raspberry', ':O': 'surprised',
          ':-@': 'shocked', ':@': 'shocked',':-$': 'confused', ':\\': 'annoyed', 
          ':#': 'mute', ':X': 'mute', ':^)': 'smile', ':-&': 'confused', '$_$': 'greedy',
          '@@': 'eyeroll', ':-!': 'confused', ':-D': 'smile', ':-0': 'yell', 'O.o': 'confused',
          '<(-_-)>': 'robot', 'd[-_-]b': 'dj', ":'-)": 'sadsmile', ';)': 'wink', 
          ';-)': 'wink', 'O:-)': 'angel','O*-)': 'angel','(:-D': 'gossip',
          '=^.^=': 'cat', ':D': 'smile', ';D': 'smile'}

def cleaner(tweet):
    
    """Function to clean text data"""
    
    
    for emoj in emojis.keys():
        if emoj in tweet:
            tweet = tweet.replace(emoj, "emoji" + emojis[emoj])
    
    tweet = tweet.lower()
    tweet = ''.join(c for c in tweet if not c.isdigit()) #remove digits

    tweet = re.sub("@[A-Za-z0-9]+","",tweet) #Remove @ sign
    tweet = re.sub(r"(?:\@|http?\://|https?\://|www)\S+", "", tweet) #Remove http links
    
    tweet = " ".join(tweet.split())
    tweet = ''.join(c for c in tweet if c not in emoji.UNICODE_EMOJI) #Remove Emojis
    
    tweet = tweet.replace("#", "").replace("_", " ") #Remove hashtag sign but keep the text
    tweet = ''.join(c for c in tweet if c not in punctuation) #remove all punctuation
    
    wordnet_lemmatizer = WordNetLemmatizer() # with use of morphological analysis of words
    tweet = [wordnet_lemmatizer.lemmatize(word) for word in nltk.word_tokenize(tweet)]
    
    
    tweet = " ".join(w for w in tweet)
    return tweet

nltk.download('all')

data2 = data1.copy()
data2['text'] = data2['text'].apply(cleaner)

data2['text']

vectorizer = CountVectorizer()
tfidf = TfidfTransformer()
clf = LogisticRegression(max_iter=10000)

pipeline = Pipeline([
('vec', vectorizer),  # strings to token integer counts
('tfidf', tfidf),  # integer counts to weighted TF-IDF scores
('classifier', clf),  # train on TF-IDF vectors 
])

X = data2['text']
y = data2['sentiment']

# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pipeline.fit(X, y) 

import pickle

filename = 'trained_model.sav'
pickle.dump(pipeline, open(filename, 'wb'))


